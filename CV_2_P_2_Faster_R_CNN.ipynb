{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umeshrawat/AI_Math_Vedas/blob/master/CV_2_P_2_Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT: It is highly recommended that this notebook be run on Google Colab itself as it contains CUDA usage, which may not be supported in the local environment of some computers."
      ],
      "metadata": {
        "id": "k2kXDYrZ8eI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem to be solved\n",
        "In this notebook, our goal is to **add artificial intelligence into a traffic system**. This traffic system has to monitor the vehicles that are passing through the intersection where it is deployed. Specifically, it has to find all vehicles, and also special vehicles like ambulances, buses, cars, motorcycles and trucks, and understand the vehicle characteristics (size, color, license plate etc). This system has a camera on a motor that rotates slowly, and can take images of the roads from different viewpoints. Some sample images from the system are shown below\n",
        "![](https://drive.google.com/file/d/1a31C5GzvbXyVsPLO9E6pdeeRnO5HN341/view?usp=share_link)\n",
        "![](https://drive.google.com/file/d/1eVjslBgj9j1l55KVJCcZxa-ADm35Di7Q/view?usp=share_link)\n",
        "\n",
        "<figure>\n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1a31C5GzvbXyVsPLO9E6pdeeRnO5HN341' />\n",
        "</td>\n",
        "<td>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1eVjslBgj9j1l55KVJCcZxa-ADm35Di7Q' />\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "</figure>\n",
        "\n",
        "As an ML engineer, you are asked to find all the vehicles, their category and size in the images from the traffic system.\n",
        "\n",
        "# Our Solution\n",
        "The solution we will adopt for this problem is to train an object detection model which can predict the different vehicle categories. We will be training the Faster RCNN and YOLO models we learned in the Computer Vision 2 lecture in this exercise.\n",
        "\n"
      ],
      "metadata": {
        "id": "x6j1wY-wq2to"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnL6xLY7xEhn"
      },
      "source": [
        "# Dataset used\n",
        "To train the object detection models, we need to either collect images with bounding boxes using human annotators or use publicly available datasets. In this exercise, we will be following the latter approach. We use the Open Images Vehicles datasets. Luckily this dataset contains all the categories we are interested in. This dataset is available as a zip file `Vehicles-OpenImages.v1-416x416.coco.zip`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "Before trying this notebook, ensure that the following requirements are met.\n",
        "1. Copy the dataset file `Vehicles-OpenImages.v1-416x416.coco.zip` to your Google Drive at the path `CV-2/assets/P3`"
      ],
      "metadata": {
        "id": "JBCGG5Rcpk2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the Data\n",
        "\n",
        "## Mounting the dataset directory\n",
        "This code is required for all notebooks. We assume that the datasets are in Google Drive, and mount the directory containing dataset into a local directory. Hence we can continue reading the data as though it was present in the machine running the colab."
      ],
      "metadata": {
        "id": "E7OamrYY5uHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVaz6GrhVfXA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "assets_dir = '/content/drive/MyDrive/CV-2/assets/P3/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZa1BAIlsiHx"
      },
      "source": [
        "## Importing common python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8safcie0TTXH"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch_snippets\n",
        "from torch_snippets import Glob, Report\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-a3Y0_4EZ21"
      },
      "source": [
        "## Unzipping the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7Hf3vqOcCt"
      },
      "source": [
        "We unzip the dataset and generate labels for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlaRS8A-sEpT"
      },
      "outputs": [],
      "source": [
        "source_path = assets_dir + \"Vehicles-OpenImages.v1-416x416.coco.zip\"\n",
        "destination_dir = \"/content/Vehicles/\"\n",
        "!unzip \"{source_path}\" -d {destination_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the annotations"
      ],
      "metadata": {
        "id": "GDcPNwjbOOSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/Vehicles/train/_annotations.coco.json',) as f:\n",
        "    annotations = json.load(f)"
      ],
      "metadata": {
        "id": "4n4p5X5n4cod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load label info"
      ],
      "metadata": {
        "id": "aNzawMGR7nVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets2label = {}\n",
        "for cat in annotations['categories']:\n",
        "    targets2label[cat['id']] = cat['name']\n",
        "\n",
        "label2targets = {target: label for label, target in targets2label.items()}\n",
        "num_classes = len(targets2label)"
      ],
      "metadata": {
        "id": "W8cR9zn078db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhdqASE4FWpW"
      },
      "source": [
        "## Populate image tensors\n",
        "In the below code snippet, the image is first converted to a PyTorch tensor using torch.tensor, and then its dimensions are permuted using permute(2, 0, 1). This operation changes the order of the dimensions, so the original shape of (height, width, channels) is changed to (channels, height, width)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "7G6hAqAO86wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DzBg1uvtDLM"
      },
      "outputs": [],
      "source": [
        "def preprocess_img(img):\n",
        "    img = torch.tensor(img).permute(2, 0 ,1)\n",
        "    return img.to(device).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Dataset\n",
        "\n",
        "Datasets are the collections of your training, validation, and test data. They consist of input samples and their corresponding target labels (for supervised learning). In PyTorch, datasets are typically created using custom classes inheriting from `torch.utils.data.Dataset`. You load your data into this class, allowing easy access during training. PyTorch provides built-in datasets like MNIST, CIFAR-10, and ImageNet, but custom datasets can also be created to work with specific data."
      ],
      "metadata": {
        "id": "h2auhwi074gq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQPf61V3Fl9f"
      },
      "source": [
        "### Creating a custom dataset class\n",
        "\n",
        "Creating a custom dataset class in PyTorch offers significant advantages in data management and flexibility for various machine learning tasks. By defining a custom dataset class, researchers and developers can handle diverse data formats, pre-processing steps, and data augmentations in a unified and organized manner. This ensures that data loading, transformation, and augmentation are seamlessly integrated into the model training pipeline.\n",
        "\n",
        "Custom dataset classes can be used with PyTorch's DataLoader, which allows batch processing, parallel data loading, and shuffling, optimizing the data loading process for improved training efficiency. Additionally, custom dataset classes facilitate the incorporation of specific data splits (e.g., train, validation, test) and enable seamless integration with various PyTorch models.\n",
        "\n",
        "Moreover, custom dataset classes are especially beneficial when working with datasets that may not adhere to conventional formats, such as medical images, audio data, or custom annotation formats. By implementing a custom dataset class, developers can tailor data loading and pre-processing to suit their specific needs and ensure compatibility with the chosen model architecture.\n",
        "\n",
        "Overall, the creation of a custom dataset class empowers researchers and developers to efficiently handle complex data, customize data transformations, and seamlessly integrate their datasets with PyTorch's data handling utilities, thereby streamlining the model training process and enhancing the overall performance of machine learning models.\n",
        "\n",
        "Below we have a custom dataset class to handle the Vehicles Dataset we have unzipped above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QVBqKjktC-r"
      },
      "outputs": [],
      "source": [
        "class VehicleDetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_path, std=False):\n",
        "        super(VehicleDetectionDataset, self).__init__()\n",
        "        self.images_path = Glob(images_path+\"*jpg\")\n",
        "        self.std = std\n",
        "        with open(images_path+'_annotations.coco.json',) as f:\n",
        "            self.annotations = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # First get the path, filename and id of the idx th image.\n",
        "        file_path = str(self.images_path[idx])\n",
        "        file_name = file_path.split(\"/\")[4]\n",
        "        img_id = None\n",
        "        for img_name in self.annotations['images']:\n",
        "            if img_name['file_name'] == file_name:\n",
        "                img_id = img_name['id']\n",
        "\n",
        "        # Load the box information of the idx th image.\n",
        "        bbox, areas, iscrowd, labels = [], [], [], []\n",
        "        for box in self.annotations['annotations']:\n",
        "            if box['image_id'] == img_id:\n",
        "                bbox.append(box['bbox'])\n",
        "                areas.append(box['area'])\n",
        "                iscrowd.append(box['iscrowd'])\n",
        "                labels.append(box['category_id'])\n",
        "\n",
        "        # Load the image.\n",
        "        img = cv2.imread(str(file_path), cv2.IMREAD_UNCHANGED)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "\n",
        "        # Convert images and box to np arrays and then to Torch tensors.\n",
        "        bbox = np.array(bbox)\n",
        "        areas = np.array(areas)\n",
        "        iscrowd = np.array(iscrowd)\n",
        "        labels = np.array(labels)\n",
        "        bbox[:, 2] = bbox[:, 0] + bbox[:, 2]\n",
        "        bbox[:, 3] = bbox[:, 1] + bbox[:, 3]\n",
        "        bbox = torch.from_numpy(bbox)\n",
        "        bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
        "        if self.std:\n",
        "            img = img/255.0\n",
        "        target = {}\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
        "        image_id = torch.tensor([idx])\n",
        "        areas = torch.as_tensor(areas.astype(np.float), dtype=torch.double)\n",
        "\n",
        "        target[\"boxes\"] = bbox\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = areas\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        img = np.array(img)\n",
        "        if img.shape[0] != 3:\n",
        "            img = img.reshape(3, img.shape[0], img.shape[1])\n",
        "\n",
        "        img = torch.from_numpy(img)\n",
        "        img = torch.as_tensor(img, dtype=torch.double)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = VehicleDetectionDataset(\"/content/Vehicles/train/\")\n",
        "\n",
        "# Visualize the 300th sample.\n",
        "img, target = dataset.__getitem__(300)\n",
        "\n",
        "img = img.reshape(img.shape[1], img.shape[2], 3)\n",
        "img = torch.as_tensor(img, dtype=torch.int)\n",
        "print(img.shape, target[\"boxes\"], target[\"labels\"])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.imshow(img)\n",
        "for lab in target['boxes']:\n",
        "    rect = patches.Rectangle((lab[0], lab[1]), lab[2]-lab[0], lab[3]-lab[1],\n",
        "                             linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.annotate(targets2label[int(target[\"labels\"][0])],(lab[0], lab[1]),\n",
        "                color='red', fontsize=15,backgroundcolor=\"w\")\n",
        "\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m4OjxsoG8eV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jzJ_CDRHGLp"
      },
      "source": [
        "### Creating the Torch Dataset and Dataloader.\n",
        "\n",
        "Dataloaders, are utilities that enable efficient data loading and batching. They take a dataset as input and allow users to define batch sizes, shuffle the data, and apply transformations to the samples. Dataloaders are especially useful when dealing with large datasets, as they enable the model to process data in small batches, reducing memory requirements and speeding up training. They are key components in PyTorch that facilitate data handling and preparation for machine learning tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dataset = VehicleDetectionDataset(\"/content/Vehicles/train/\", std=True)\n",
        "val_dataset = VehicleDetectionDataset(\"/content/Vehicles/valid/\", std=True)\n",
        "test_dataset = VehicleDetectionDataset(\"/content/Vehicles/test/\", std=True)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=4, shuffle=True, num_workers=0,\n",
        "        collate_fn = collate_fn)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=1, shuffle=False, num_workers=0,\n",
        "        collate_fn = collate_fn)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=1, shuffle=False, num_workers=0,\n",
        "        collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "Rk0CnEfu8VRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swKwqNhjJBVY"
      },
      "source": [
        "# Defining the Faster RCNNModel\n",
        "We will not be implementing the Faster RCNN model from scratch. Instead, we will load the architecture definition provided by PyTorch in the torchvision library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    \"\"\"\n",
        "    Returns a pre-trained Faster R-CNN model based on ResNet-50 backbone with a modified box predictor for the specified number of classes.\n",
        "\n",
        "    Returns:\n",
        "    torch.nn.Module: A pre-trained Faster R-CNN model based on ResNet-50 backbone with the specified number of classes.\n",
        "    \"\"\"\n",
        "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # Load a pre-trained Faster R-CNN model with ResNet-50 backbone\n",
        "\n",
        "    # Remove the existing box_predictor since we have a different number of categories.\n",
        "    # To do this, first get the input to the box predictor, and define our own box_predictor\n",
        "    # processing that input.\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n",
        "    return model"
      ],
      "metadata": {
        "id": "I34S3nL99CK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klf2ZJlbsLAp"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "imgs, targets = next(iter(train_dataloader))\n",
        "\n",
        "imgs = list(img.to(device).float() for img in imgs)\n",
        "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "model = get_model().to(device).float()\n",
        "model.eval()\n",
        "model(imgs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model and optimizer\n",
        "faster_rcnn_model = get_model().to(device) # Get the pre-trained Faster R-CNN model and move it to the specified device"
      ],
      "metadata": {
        "id": "nC2Pu4Rt9uFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "Optimizers are algorithms that adjust the model's parameters during training to minimize the loss function. Common optimizers include SGD (Stochastic Gradient Descent), Adam, and RMSprop."
      ],
      "metadata": {
        "id": "bt2Er93Q96g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faster_rcnn_optimizer = torch.optim.SGD(faster_rcnn_model.parameters(), lr=0.005, weight_decay=5e-4, momentum=0.9) # Define the optimizer with SGD"
      ],
      "metadata": {
        "id": "0JVcGGn39w-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scheduler\n",
        "\n",
        "A scheduler adjusts the learning rate dynamically during training, allowing fine-tuning.\n",
        "\n",
        "Cosine Annealing: The learning rate starts high and is annealed down to a minimum value following a cosine curve. It helps the model explore the search space broadly at the beginning of training and then refine the search space as it converges.\n",
        "\n",
        "T_max: This parameter defines the total number of iterations it takes to complete one cycle of the cosine function. The learning rate will follow a cosine curve for the first T_max iterations and then restart the cycle.\n",
        "\n",
        "Here's a conceptual explanation:\n",
        "\n",
        "At the start of training, the learning rate is relatively high, allowing the model to explore a larger area of the loss landscape.\n",
        "As training progresses (over the T_max iterations), the learning rate decreases following a cosine curve.\n",
        "When T_max iterations are completed, the learning rate is at its minimum.\n",
        "The scheduler then restarts the cosine curve, and the learning rate starts to increase again, allowing the model to explore broadly for the next cycle.\n",
        "This approach often helps models converge more efficiently by first exploring broadly and then refining their parameters as training progresses."
      ],
      "metadata": {
        "id": "cXu9cz2P_Zje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faster_rcnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(faster_rcnn_optimizer, T_max = 200)"
      ],
      "metadata": {
        "id": "nOhZJ_ws_ai_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el8nAQNBLV44"
      },
      "source": [
        "### Train and evaluation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9jN21x0zOa0"
      },
      "source": [
        "The train_batch and validate_batch methods are used to train each batch of inputs from the training and validation dataloaders respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DQHqhTDwjF2"
      },
      "outputs": [],
      "source": [
        "def train_batch(epoch, model, optim, log):\n",
        "    \"\"\"\n",
        "    Train the model on a single epoch.\n",
        "\n",
        "    Parameters:\n",
        "        batch (tuple): A tuple containing the input images and their corresponding target annotations.\n",
        "        model (torch.nn.Module): The neural network model to be trained.\n",
        "        optim (torch.optim.Optimizer): The optimizer used for updating the model's parameters.\n",
        "        log: The log object used to generate the Report for the training and validation phase\n",
        "    \"\"\"\n",
        "    print(\"epoch \", epoch)\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        N = len(train_dataloader) # Number of batches in the training dataloader\n",
        "        imgs, targets = batch # Unpack the input images and their corresponding target annotations from the batch\n",
        "        imgs = list(img.to(device).float() for img in imgs) # Move the input images to the specified device\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets] # Move the target annotations to the specified device\n",
        "        optim.zero_grad() # Zero the gradients in the optimizer\n",
        "        losses = model(imgs, targets) # Forward pass: Get the output losses from the model\n",
        "        loss = sum(loss for loss in losses.values()) # Compute the total loss by summing individual losses\n",
        "        loss.backward() # Backward pass: Compute gradients of the total loss with respect to model parameters\n",
        "        optim.step() # Update the model parameters using the optimizer\n",
        "        # Extract individual losses from the total loss for logging\n",
        "        classifier_loss, regression_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in\n",
        "                                                                  ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
        "        # Record the training loss and individual losses to the log\n",
        "        log.record(e + (i+1)/N, train_loss=loss.item(), train_classifier_loss=classifier_loss.item(),\n",
        "                   train_regression_loss=regression_loss.item(), train_loss_objectness=loss_objectness.item(),\n",
        "                   train_loss_rpn_box_reg = loss_rpn_box_reg.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate_batch(epoch, model, log):\n",
        "    \"\"\"\n",
        "    Validate the model on a single batch of data without performing gradient updates.\n",
        "\n",
        "    Parameters:\n",
        "        batch (tuple): A tuple containing the input images and their corresponding target annotations.\n",
        "        model (torch.nn.Module): The neural network model to be validated.\n",
        "        log: The log object used to generate the Report for the training and validation phase\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        N = len(val_dataloader)\n",
        "        imgs, targets = batch\n",
        "        imgs = list(img.to(device).float() for img in imgs)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        losses = model(imgs, targets)\n",
        "\n",
        "        loss = sum(loss for loss in losses.values())\n",
        "        # Extract individual losses from the total loss for logging\n",
        "        classifier_loss, regression_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in\n",
        "                                                                  ['loss_classifier', 'loss_box_reg',\n",
        "                                                                   'loss_objectness', 'loss_rpn_box_reg']]\n",
        "        # Record the validation loss and individual losses to the log\n",
        "        log.record(e + (i+1)/N, val_loss=loss.item(), val_classifier_loss=classifier_loss.item(),\n",
        "                   val_regression_loss=regression_loss.item(), val_loss_objectness=loss_objectness.item(),\n",
        "                   val_loss_rpn_box_reg = loss_rpn_box_reg.item())\n"
      ],
      "metadata": {
        "id": "0TFYggv_9J2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0obM2uVLjth"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqWkSnlJwr9v"
      },
      "source": [
        "In Fast R-CNN object detection, the model typically computes and returns four losses:\n",
        "\n",
        "Classification Loss: This loss measures the accuracy of the predicted class labels for each object region. It is usually computed using a classification loss function such as cross-entropy loss.\n",
        "\n",
        "Localization Loss: This loss measures the accuracy of the predicted bounding box coordinates for each object region. It is commonly calculated using a regression loss function such as smooth L1 loss.\n",
        "\n",
        "Objectness Loss: This loss is specific to region proposal networks (RPN) used in the Faster R-CNN framework. It measures the accuracy of the predicted objectness scores, which indicate the likelihood of a region containing an object.\n",
        "\n",
        "RPN Localization Loss: This loss is also specific to the Faster R-CNN framework and is applicable only for the region proposal network (RPN). It measures the accuracy of the predicted bounding box coordinates for the proposed regions.\n",
        "\n",
        "These losses are combined and optimized during training to guide the model in learning accurate object detection and localization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9H7oobnk_Ha"
      },
      "source": [
        "Note: The below code will take a long time to run on the basic GPU and so in order to save time you can load the model weights and play around with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiwTNSf9sOSQ"
      },
      "outputs": [],
      "source": [
        "n_epochs = 1\n",
        "log = Report(n_epochs) # Create a Report object to store and visualize training progress\n",
        "for e in range(n_epochs):\n",
        "    # Training phase\n",
        "    train_batch(epoch=e, model=faster_rcnn_model.float(), optim=faster_rcnn_optimizer, log=log) # Train the model on a single epoch\n",
        "    # Validation phase\n",
        "    validate_batch(epoch=e, model=faster_rcnn_model.float(), log=log)\n",
        "    faster_rcnn_scheduler.step()\n",
        "    log.report_avgs(e+1) # Report the average losses for the current epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing validation loss on a previously trained model\n",
        "\n",
        "Since the training takes a long time, we will load a previously trained model and do evaluation."
      ],
      "metadata": {
        "id": "B5iiJecly29S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = assets_dir +'faster_rcnn_model_v2.pt'\n",
        "\n",
        "loaded_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)\n",
        "# Load the saved state dictionary\n",
        "saved_state_dict = torch.load(file_path)\n",
        "\n",
        "n_epochs = 2\n",
        "log = Report(n_epochs) # Create a Report object to store and visualize training progress\n",
        "for e in range(n_epochs):\n",
        "    # Validation phase\n",
        "    validate_batch(epoch=e, model=faster_rcnn_model.float(), log=log)\n",
        "    log.report_avgs(e+1)\n",
        "\n",
        "# Plot the training and validation loss curves\n",
        "log.plot_epochs(['val_loss'])"
      ],
      "metadata": {
        "id": "m05xYJoOBP5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a trained model"
      ],
      "metadata": {
        "id": "9uSAAiyrzFD1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOIgkBXSk05a"
      },
      "outputs": [],
      "source": [
        "model_save_path = assets_dir + \"faster_rcnn_model_v2.pt\"\n",
        "torch.save(faster_rcnn_model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bapKt-vNkmA"
      },
      "source": [
        "### Validating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWf7E6wgyYC5"
      },
      "source": [
        "After training the model we can run the validation data on this and see how it performs. We are taking the first 5 images. You can play around with the number and see the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import nms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lbWJ5-X_Bdtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBaFLKjysVzR"
      },
      "outputs": [],
      "source": [
        "def sample_prediction(model, idx):\n",
        "    model.eval()\n",
        "    cpu = torch.device(\"cpu\")\n",
        "    model.to(cpu)\n",
        "    sample_image = test_dataset.__getitem__(idx)[0]\n",
        "    groundtruth_boxes = test_dataset.__getitem__(idx)[1][\"boxes\"]\n",
        "    sample_image = torch.unsqueeze(sample_image, 0)\n",
        "    sample_image.to(cpu)\n",
        "    outputs = model(sample_image)\n",
        "    outputs = [{k: v for k, v in t.items()} for t in outputs]\n",
        "    boxes = outputs[0][\"boxes\"].detach().numpy()\n",
        "    scores = outputs[0][\"scores\"].detach().numpy()\n",
        "    labels = outputs[0][\"labels\"].detach().numpy()\n",
        "    sample_image = sample_image.reshape(sample_image.shape[2],sample_image.shape[3], 3)\n",
        "    return sample_image, boxes, labels, scores, groundtruth_boxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nms():\n",
        "    iou = 0.99\n",
        "    iou_threshold = 1 - iou\n",
        "    NMS = torchvision.ops.nms(torch.tensor(boxes), torch.tensor(scores), iou_threshold)\n",
        "    best_boxes, best_scores, best_labels = [], [], []\n",
        "    for nms in NMS:\n",
        "        best_boxes.append(boxes[nms])\n",
        "        best_scores.append(scores[nms])\n",
        "        best_labels.append(labels[nms])\n",
        "    return best_boxes, best_scores, best_labels"
      ],
      "metadata": {
        "id": "IpPk0sG0BbXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kMDceB7O66f"
      },
      "outputs": [],
      "source": [
        "loaded_model.eval()\n",
        "loaded_model.double()\n",
        "for i in range(10):\n",
        "    sample_image, boxes, labels, scores, groundtruth_boxes = sample_prediction(loaded_model, i)\n",
        "    best_boxes, best_scores, best_labels = nms()\n",
        "    fig, ax = plt.subplots(figsize=(12,18))\n",
        "    ax.imshow(sample_image)\n",
        "    for box, score, label in zip(best_boxes, best_scores, best_labels):\n",
        "        rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
        "                                linewidth=1, edgecolor='r', facecolor='none')\n",
        "        ann_text = targets2label[int(label)]+\" \"+str(score)[:4]\n",
        "        ax.annotate(ann_text,(box[0], box[1]), color='red', fontsize=15,backgroundcolor=\"w\")\n",
        "        ax.add_patch(rect)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}