{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umeshrawat/AI_Math_Vedas/blob/master/CV_3_Assignment_Question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "46WCCBk6yiUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REOm97JMyZR2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import vgg16\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data From Google Drive"
      ],
      "metadata": {
        "id": "HLH4k3yGy-W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "assets_dir = '/content/drive/MyDrive/CV-3/MCQs and Assignment/assets/'"
      ],
      "metadata": {
        "id": "w0srz8x8y9hC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08299175-a2d0-46fd-d9d0-53690c4c4bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you are tasked with finding the top-k similar images of every image in the test set of the AT&T dataset, using a Siamese Network with a VGG backbone, but instead of the Contrastive Loss Function learnt in class, you will be need to use the Triplet Loss Function. You need to use Cosine Similarity to find the Top-K similar images for each image and display them.\n",
        "\n",
        "Siamese Networks is something which is not covered in the class yet but feel free to to read up about it and how it can be used for Image Similarity.\n",
        "\n"
      ],
      "metadata": {
        "id": "VeVxVJPlzIMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the dataset"
      ],
      "metadata": {
        "id": "jt5sMa97ypPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the zip file you want to unzip\n",
        "zip_file_path = assets_dir + \"AT&T.zip\"\n",
        "\n",
        "# Target folder where you want to extract the contents\n",
        "target_folder = \"/content/dataset\"\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_folder)\n",
        "\n",
        "# assert the folder exists\n",
        "assert os.path.isdir(target_folder + '/AT&T'), \"The unzipped folder cannot be found\""
      ],
      "metadata": {
        "id": "KFCIbJFNyo1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dir = \"/content/dataset/AT&T/train/\"\n",
        "testing_dir = \"/content/dataset/AT&T/test/\""
      ],
      "metadata": {
        "id": "MOh2kpiGzyML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Siamese Network with VGG backbone\n",
        "\n",
        "**Task**: The below class defines a VGG backbone for the Siamese Network where you will need to replace the last layer from the VGG model and then add a few dense layers to it along with regularization using Dropout and Batch Normalization."
      ],
      "metadata": {
        "id": "qx4p6jy50-J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Siamese Network with VGG-16 backbone for image similarity comparison.\n",
        "\n",
        "        The network consists of a VGG-16 model with the last layer removed, followed by fully connected layers\n",
        "        to extract features from input images. The extracted features are then used to compute image similarity.\n",
        "\n",
        "        \"\"\"\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        vgg = vgg16(pretrained=True)\n",
        "        # Get the list of layers in the VGG-16 model.\n",
        "        layers = list(vgg.children())\n",
        "        # Remove the last layer from the list of layers.\n",
        "        layers = layers[:-1]\n",
        "\n",
        "        # Create a new model from the list of layers.\n",
        "        self.backbone = torch.nn.Sequential(*layers) # This is the VGG backbone\n",
        "        self.fc1 = ## TODO - write your solution here\n",
        "        self.bn1 = ## TODO - write your solution here\n",
        "        self.bn2 = ## TODO - write your solution here\n",
        "\n",
        "    def forward_on_single_image(self, x):\n",
        "        \"\"\"\n",
        "        Perform forward pass for a single input image.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): The input image tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output feature tensor obtained after passing through the network layers.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.fc0(x)\n",
        "        # flatten the features\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2, input3):\n",
        "        \"\"\"\n",
        "        Perform forward pass for a pair of input images.\n",
        "\n",
        "        Parameters:\n",
        "            input1 (torch.Tensor): The first input image tensor.\n",
        "            input2 (torch.Tensor): The second input image tensor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the output feature tensors obtained after passing each input image\n",
        "                   through the network.\n",
        "\n",
        "        \"\"\"\n",
        "        output1 = self.forward_on_single_image(input1)\n",
        "        output2 = self.forward_on_single_image(input2)\n",
        "        output3 = self.forward_on_single_image(input3)\n",
        "        return output1, output2, output3\n"
      ],
      "metadata": {
        "id": "Q4TkpGNO051T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Siamese Dataset\n",
        "**Task**: You need to create a dataset from the images where it should contain three faces, in which 2 are similar and the 3rd is dissimilar thus helping train the Siamese Network to discern the differences between the 2 images that are different and the similarity between the 2 images which are similar."
      ],
      "metadata": {
        "id": "R8CNBzJI3A6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Siamese Network Dataset\n",
        "class SiameseNetworkDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for Siamese Network training.\n",
        "\n",
        "    This dataset randomly samples pairs of images from the given input dataset, along with their corresponding labels\n",
        "    indicating whether the images belong to the same class or not. It is used for training Siamese Networks which\n",
        "    learn to compare and measure similarity between two input images.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (Dataset): The original (PyTorch) dataset containing images and their labels.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"\n",
        "        Initialize the Siamese Network dataset.\n",
        "\n",
        "        Parameters:\n",
        "            dataset (Dataset): The original (PyTorch) dataset containing images and their labels.\n",
        "\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.labels = torch.arange(len(dataset))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Get a single sample from the Siamese Network dataset.\n",
        "\n",
        "        Parameters:\n",
        "            index (int): The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing three images. The first element is the anchor image (img1), the second element is\n",
        "                   the image which has the same label as the first image i.e. the positive image and the third element\n",
        "                   is the image which has a different label than the first image i.e. the negative image.\n",
        "\n",
        "        \"\"\"\n",
        "        ## TODO - write your solution here\n",
        "        # return anchor, positive, negative\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of samples in the dataset.\n",
        "\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n"
      ],
      "metadata": {
        "id": "FlZNOc_A23jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Write the forward function in the TripletLoss class that takes the three images (anchor, positive, negative) from the dataset as input and returns the calculated loss."
      ],
      "metadata": {
        "id": "oMr6zvsh30Z5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5UtegR_lXU8"
      },
      "source": [
        "### Setting the Transformations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the transformations\n",
        "transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                              transforms.ToTensor()\n",
        "                            ])"
      ],
      "metadata": {
        "id": "0mrWXfZ_lXU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY5hrrjJlXU_"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Datasets are the collections of your training, validation, and test data. They consist of input samples and their corresponding target labels (for supervised learning). In PyTorch, datasets are typically created using custom classes inheriting from `torch.utils.data.Dataset`. You load your data into this class, allowing easy access during training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageFolder(training_dir, transform=transform)\n",
        "test_dataset = ImageFolder(testing_dir, transform=transform)\n",
        "\n",
        "print(len(train_dataset), len(test_dataset))\n",
        "\n",
        "# Create Siamese datasets\n",
        "train_siamese_dataset = SiameseNetworkDataset(train_dataset)\n",
        "test_siamese_dataset = SiameseNetworkDataset(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBvHE7fWlXVA",
        "outputId": "a41ac85b-9376-48a6-f909-e3c35ad6e825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloaders\n",
        "\n",
        "Data Loaders wrap your dataset and provide functionalities for iterating through batches of data during training. They handle shuffling, batching, and parallel data loading, optimizing the data pipeline."
      ],
      "metadata": {
        "id": "IJnZp9twlXVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d465011-a612-4d38-ab7b-944f831251ab",
        "id": "RSgq7yl4lXVB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset for Siamese Network created. 370\n",
            "Test Dataset for Siamese Network created. 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "batch_size = 64\n",
        "test_batch_size = 1\n",
        "\n",
        "train_loader = DataLoader(train_siamese_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "test_loader = DataLoader(test_siamese_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "print(\"Train Dataset for Siamese Network created.\", len(train_siamese_dataset))\n",
        "print(\"Test Dataset for Siamese Network created.\", len(test_siamese_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "Loss functions measure the difference between the predicted output and the actual target values. Common loss functions include Cross-Entropy Loss for classification tasks and Mean Squared Error for regression tasks."
      ],
      "metadata": {
        "id": "Y81HMOy1k3tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Triplet Loss Function\n",
        "\n",
        "The Triplet Loss function is a key component in training Siamese Networks and related architectures for learning similarity and dissimilarity among data points. Similar to the Contrastive Loss, the Triplet Loss aims to shape the embedding space in a way that similar samples are closer together, while dissimilar samples are pushed farther apart.\n",
        "\n",
        "In the context of the Triplet Loss, each training sample is referred to as an \"anchor.\" For each anchor, we identify a \"positive\" sample (similar to the anchor) and a \"negative\" sample (dissimilar to the anchor). The goal of the Triplet Loss is to ensure that the distance between the anchor and the positive sample is smaller than the distance between the anchor and the negative sample by a certain margin.\n",
        "\n",
        "Mathematically, for an anchor sample \\(A\\), a positive sample \\(P\\), and a negative sample \\(N\\), the Triplet Loss is computed as:\n",
        "\n",
        "$$L(A, P, N) = max ( | f(A) - f(P) |^2 - | f(A) - f(N) |^2 + margin, 0 )$$\n",
        "\n",
        "Here, f(.) represents the embedding function learned by the neural network, and \\(| . |\\) denotes the Euclidean distance between the embedded vectors. The margin is a hyperparameter that specifies the minimum desired separation between positive and negative samples.\n",
        "\n",
        "In summary, the Triplet Loss guides the network to learn embeddings in such a way that the positive sample is pulled closer to the anchor while the negative sample is pushed away. This encourages the network to map similar samples together in the embedding space and dissimilar samples apart, resulting in meaningful representations that capture the inherent structure of the data's similarity relationships."
      ],
      "metadata": {
        "id": "wa0Nvi-N3tdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet Loss function for training Siamese Networks with triplet sampling.\n",
        "\n",
        "    The Triplet Loss encourages the model to minimize the distance between the anchor and the positive example\n",
        "    while maximizing the distance between the anchor and the negative example. This helps in learning a suitable\n",
        "    embedding space where similar examples are closer and dissimilar examples are farther apart.\n",
        "\n",
        "    Parameters:\n",
        "        margin (float): The margin value that defines the desired separation between positive and negative pairs.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        \"\"\"\n",
        "        Compute the Triplet Loss based on the anchor, positive, and negative examples.\n",
        "\n",
        "        Parameters:\n",
        "            anchor (torch.Tensor): Embeddings of the anchor examples.\n",
        "            positive (torch.Tensor): Embeddings of the positive examples.\n",
        "            negative (torch.Tensor): Embeddings of the negative examples.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Computed Triplet Loss.\n",
        "\n",
        "        \"\"\"\n",
        "        ## TODO - write your solution here\n",
        "        # return loss"
      ],
      "metadata": {
        "id": "22RummGD3lqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "triplet_loss = TripletLoss()"
      ],
      "metadata": {
        "id": "CZVyrEuaxgNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create an instance of the Siamese Network with VGG16 backbone\n",
        "model = SiameseNetwork().to(device)"
      ],
      "metadata": {
        "id": "DTPIOw32xik3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer\n",
        "\n",
        "Optimizers are algorithms that adjust the model's parameters during training to minimize the loss function. Common optimizers include SGD (Stochastic Gradient Descent), Adam, and RMSprop."
      ],
      "metadata": {
        "id": "U8PeBkBTlvkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "8sNXYCfxkGgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(epoch, model, optimizer, loss_history):\n",
        "    print(\"epoch \", epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "        anchor, positive, negative = batch\n",
        "        anchor = anchor.to(device) # Move the anchor image to the device\n",
        "        positive = positive.to(device) # Move the positive image to the device\n",
        "        negative = negative.to(device) # Move the negative image to the device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients to prevent accumulation from previous iterations\n",
        "        output_anchor, output_pos, output_neg = model(anchor, positive, negative)  # Forward pass: get the output feature vectors for all three images\n",
        "        loss = triplet_loss(output_anchor, output_pos, output_neg) # Calculate the Triplet Loss\n",
        "        loss.backward()  # Backward pass: compute gradients of the loss with respect to model parameters\n",
        "        optimizer.step()  # Update the model's parameters using the computed gradients\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Train Loss: %.3f'\n",
        "                         % (train_loss/(batch_idx+1)))\n",
        "    loss_history.append(train_loss)\n"
      ],
      "metadata": {
        "id": "bLBF7K3MxuAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_batch(epoch, model, loss_history):\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            anchor, positive, negative = batch\n",
        "            anchor = anchor.to(device) # Move the anchor image to the device\n",
        "            positive = positive.to(device) # Move the positive image to the device\n",
        "            negative = negative.to(device) # Move the negative image to the device\n",
        "\n",
        "            output_anchor, output_pos, output_neg = model(anchor, positive, negative)  # Forward pass: get the output feature vectors for all three images\n",
        "            loss = triplet_loss(output_anchor, output_pos, output_neg) # Calculate the Triplet Loss\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    print('Val Loss: %.3f'\n",
        "                 % (test_loss/(batch_idx+1)))\n",
        "    loss_history.append(test_loss)"
      ],
      "metadata": {
        "id": "Y5mpirD4xxLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "KLGJxc9Y4iPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to track training progress\n",
        "train_loss_history = [] # List to store the loss value during training\n",
        "val_loss_history = [] # List to store the loss value during validation\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_batch(epoch, model, optimizer, train_loss_history)\n",
        "    validate_batch(epoch, model, val_loss_history)"
      ],
      "metadata": {
        "id": "KsvwNNmm4fKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the loss"
      ],
      "metadata": {
        "id": "gDE1vh8X5E3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the loss values using Matplotlib (optional)\n",
        "epochs = list(range(1, len(train_loss_history) + 1))\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(epochs, train_loss_history, label='Train Loss')\n",
        "plt.plot(epochs, val_loss_history, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JyLZiQrA5Bb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Model"
      ],
      "metadata": {
        "id": "8JosmjdJ5HdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Euclidean distance is a commonly used metric to quantify the similarity between two feature vectors in image analysis. It calculates the straight-line distance between the points in the feature space. Smaller distances indicate higher similarity, while larger distances indicate dissimilarity. The Euclidean distance metric is intuitive and straightforward to compute, making it widely used in various applications such as image retrieval and clustering."
      ],
      "metadata": {
        "id": "CcN6t9QZ5TEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DRJSZ24CyUAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Note: Each batch contains only 1 image for test loader as set above\n",
        "    for i, batch in enumerate(test_loader):\n",
        "\n",
        "        # setting x0 to the first image which will be used to compare against other 9 images\n",
        "        if i == 0:\n",
        "          x0,_,_ = batch\n",
        "          image1 = x0\n",
        "          x0 = x0.to(device)\n",
        "          continue\n",
        "\n",
        "        # Limiting to 10 images\n",
        "        if i == 10:\n",
        "          break\n",
        "\n",
        "        _,x1,_ = batch\n",
        "        image2 = x1\n",
        "        x1 = x1.to(device)\n",
        "        # Concatenate two images side by side for visualization\n",
        "        concatenated = torch.cat((image1,image2),0)\n",
        "        # Forward pass through the Siamese Network to get the output feature vectors for both images\n",
        "        output1 = model.forward_on_single_image(x0)\n",
        "        output2 = model.forward_on_single_image(x1)\n",
        "\n",
        "        # detach the tensor and move it to the CPU\n",
        "        x1 = x1.detach().cpu()\n",
        "\n",
        "        # Calculate the Euclidean distance between the output feature vectors\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        concatenated_img = torchvision.utils.make_grid(concatenated)\n",
        "        concatenated_img_text = 'Dissimilarity: {:.2f}'.format(euclidean_distance.item())\n",
        "\n",
        "        npimg = concatenated_img.numpy()\n",
        "        plt.axis(\"off\")\n",
        "        if concatenated_img_text:\n",
        "            plt.text(75, 8, concatenated_img_text, style='italic',fontweight='bold',\n",
        "                bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    x0 = x0.detach().cpu()"
      ],
      "metadata": {
        "id": "FhA3BLY95MxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity is a widely employed metric for assessing the similarity between two feature vectors in image analysis and other domains. It quantifies the cosine of the angle between two vectors in the feature space. Higher cosine similarity values indicate greater alignment in direction and orientation, implying higher similarity between the vectors.\n",
        "\n",
        "Compared to Euclidean distance, which measures the straight-line distance between points, Cosine Similarity focuses on the angle between vectors, disregarding their magnitudes. This means that even if the vectors have different lengths, they can still exhibit high cosine similarity if they are pointing in the same general direction.\n",
        "\n",
        "Mathematically, the Cosine Similarity between two vectors \\(A\\) and \\(B\\) is computed as:\n",
        "\n",
        "Cosine Similarity(A, B) = A . B / (|A\\| * \\|B\\|)\n",
        "\n",
        "Where \\(A . B\\) represents the dot product of the two vectors, and \\(\\|A\\|\\) and \\(\\|B\\|\\) represent the magnitudes (or lengths) of the respective vectors.\n",
        "\n",
        "Cosine Similarity values range between -1 and 1. A Cosine Similarity of 1 indicates that the vectors are perfectly aligned and have the same orientation. A Cosine Similarity of -1 indicates that the vectors are perfectly aligned but have opposite orientations. A Cosine Similarity of 0 implies that the vectors are orthogonal, indicating no similarity in direction.\n",
        "\n",
        "Cosine Similarity finds use in diverse applications like text analysis, recommendation systems, and image retrieval. Its ability to measure the direction of similarity while ignoring magnitude variations makes it particularly suited for scenarios where magnitude differences are not as relevant as the alignment of features."
      ],
      "metadata": {
        "id": "sc76e_wr5Xef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Write the code to perform cosine_similarity between a random image with any 10 images."
      ],
      "metadata": {
        "id": "e81-Yzfj5edo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Cosine Similarity\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Note: Each batch contains only 1 image for test loader as set above\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        # setting x0 to the first image which will be used to compare against other 9 images\n",
        "        if i == 0:\n",
        "          x0,_,_ = batch\n",
        "          image1 = x0\n",
        "          x0 = x0.to(device)\n",
        "          continue\n",
        "\n",
        "        # Limiting to 10 images\n",
        "        if i == 10:\n",
        "          break\n",
        "\n",
        "        _,x1,_ = batch\n",
        "        image2 = x1\n",
        "        x1 = x1.to(device)\n",
        "\n",
        "        # Concatenate two images side by side for visualization\n",
        "        concatenated = torch.cat((image1,image2),0)\n",
        "\n",
        "        # Forward pass through the Siamese Network to get the output feature vectors for both images\n",
        "        output1 = model.forward_on_single_image(x0)\n",
        "        output2 = model.forward_on_single_image(x1)\n",
        "\n",
        "        # detach the tensor and move it to the CPU\n",
        "        x1 = x1.detach().cpu()\n",
        "\n",
        "        # Calculate the Cosine Similarity between the output feature vectors\n",
        "        cosine_similarity = #TODO: Write your solution here\n",
        "        concatenated_img = torchvision.utils.make_grid(concatenated)\n",
        "        concatenated_img_text = 'Similarity: {:.8f}'.format(cosine_similarity.item())\n",
        "\n",
        "        npimg = concatenated_img.numpy()\n",
        "        plt.axis(\"off\")\n",
        "        if concatenated_img_text:\n",
        "            plt.text(75, 8, concatenated_img_text, style='italic',fontweight='bold',\n",
        "                bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    x0 = x0.detach().cpu()"
      ],
      "metadata": {
        "id": "4LpIqlxfkVkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Top-5 Images"
      ],
      "metadata": {
        "id": "7BJJGktu56iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Write the code to find the top-k images for all images in the test folder using cosine_similarity."
      ],
      "metadata": {
        "id": "3AnQRVEK7gt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - Write your solution here to find and display the top-5 similar images for each image in the test folder using Cosine Similarity"
      ],
      "metadata": {
        "id": "1Fqv7EV_57Ya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}